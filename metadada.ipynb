{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8152e1dd-4d2b-4187-8cdb-293876f7834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3\n",
      "Requirement already satisfied: pip in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (25.1.1)\n",
      "Requirement already satisfied: PyPDF2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: pytesseract in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: PyMuPDF in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: spacy in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (3.8.6)\n",
      "Requirement already satisfied: transformers in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (3.1.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from srsly<3.0.0,>=2.4.3->spacy) (5.10.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: filelock in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n",
      "Requirement already satisfied: nltk in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: textstat in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (0.7.7)\n",
      "Requirement already satisfied: click in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pyphen in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from textstat) (1.0.32)\n",
      "Requirement already satisfied: setuptools in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from textstat) (72.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from cmudict->textstat) (8.6.1)\n",
      "Requirement already satisfied: importlib-resources>=5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
      "Requirement already satisfied: streamlit in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (1.46.0)\n",
      "Requirement already satisfied: pandas in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.1.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<26,>=20 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.31.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: jinja2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.43.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/navya/miniconda3/envs/my_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install numpy==1.24.3\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Document processing packages\n",
    "!pip install PyPDF2 python-docx pytesseract pillow PyMuPDF\n",
    "\n",
    "# NLP and ML packages\n",
    "!pip install spacy transformers sentence-transformers\n",
    "!pip install nltk textstat\n",
    "\n",
    "# Web framework\n",
    "!pip install streamlit pandas\n",
    "\n",
    "# Download spaCy language model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bffe32-766f-4b03-ac82-e682830473d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Document processing\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# NLP and ML\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "import textstat\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd87d9eb-15aa-4189-b163-b8fa14b67c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataGenerator:\n",
    "    def __init__(self, quick_start=False):\n",
    "        \"\"\"\n",
    "        Initialize the Metadata Generator\n",
    "        \n",
    "        Args:\n",
    "            quick_start (bool): If True, skip heavy ML model loading\n",
    "        \"\"\"\n",
    "        self.quick_start = quick_start\n",
    "        self.nlp = None\n",
    "        self.summarizer = None\n",
    "        \n",
    "        if not quick_start:\n",
    "            self._load_models()\n",
    "        \n",
    "        print(f\"‚úÖ MetadataGenerator initialized ({'Quick Start' if quick_start else 'Full System'} mode)\")\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load NLP models with error handling\"\"\"\n",
    "        try:\n",
    "            # Load spaCy model\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            print(\"‚úÖ spaCy model loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load spaCy model: {e}\")\n",
    "            self.nlp = None\n",
    "        \n",
    "        try:\n",
    "            # Load summarization pipeline\n",
    "            self.summarizer = pipeline(\"summarization\", \n",
    "                                     model=\"facebook/bart-large-cnn\",\n",
    "                                     max_length=150, \n",
    "                                     min_length=50)\n",
    "            print(\"‚úÖ Summarization model loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load summarization model: {e}\")\n",
    "            self.summarizer = None\n",
    "    \n",
    "    def extract_text_from_pdf(self, file_path):\n",
    "        \"\"\"Extract text from PDF with OCR fallback\"\"\"\n",
    "        text = \"\"\n",
    "        \n",
    "        try:\n",
    "            # Try PyMuPDF first\n",
    "            doc = fitz.open(file_path)\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            doc.close()\n",
    "            \n",
    "            if text.strip():\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"PyMuPDF extraction failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback to PyPDF2\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "            \n",
    "            if text.strip():\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"PyPDF2 extraction failed: {e}\")\n",
    "        \n",
    "        # OCR fallback for image-based PDFs\n",
    "        try:\n",
    "            doc = fitz.open(file_path)\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                pix = page.get_pixmap()\n",
    "                img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "                text += pytesseract.image_to_string(img)\n",
    "            doc.close()\n",
    "            print(\"‚úÖ OCR extraction completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è OCR extraction failed: {e}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_text_from_docx(self, file_path):\n",
    "        \"\"\"Extract text from DOCX file\"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è DOCX extraction failed: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_text_from_file(self, file_path):\n",
    "        \"\"\"Extract text from various file formats\"\"\"\n",
    "        file_extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            return self.extract_text_from_pdf(file_path)\n",
    "        elif file_extension == '.docx':\n",
    "            return self.extract_text_from_docx(file_path)\n",
    "        elif file_extension in ['.txt', '.md']:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48983e5b-56cd-4592-9811-dff03ed6efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced NLP methods added to MetadataGenerator class\n"
     ]
    }
   ],
   "source": [
    "def classify_document_type(self, text):\n",
    "        \"\"\"Classify document type based on content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Simple rule-based classification\n",
    "        if any(word in text_lower for word in ['agreement', 'contract', 'terms', 'legal']):\n",
    "            return 'Legal'\n",
    "        elif any(word in text_lower for word in ['report', 'analysis', 'findings', 'results']):\n",
    "            return 'Report'\n",
    "        elif any(word in text_lower for word in ['manual', 'guide', 'instructions', 'how to']):\n",
    "            return 'Manual'\n",
    "        elif any(word in text_lower for word in ['proposal', 'request', 'rfp', 'bid']):\n",
    "            return 'Proposal'\n",
    "        elif any(word in text_lower for word in ['research', 'study', 'methodology', 'hypothesis']):\n",
    "            return 'Research'\n",
    "        else:\n",
    "            return 'Document'\n",
    "    \n",
    "def extract_entities(self, text):\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        if not self.nlp:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            doc = self.nlp(text[:1000000])  # Limit text size for performance\n",
    "            entities = {}\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ not in entities:\n",
    "                    entities[ent.label_] = []\n",
    "                if ent.text not in entities[ent.label_]:\n",
    "                    entities[ent.label_].append(ent.text)\n",
    "            \n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Entity extraction failed: {e}\")\n",
    "            return {}\n",
    "    \n",
    "def generate_summary(self, text):\n",
    "        \"\"\"Generate document summary\"\"\"\n",
    "        if not self.summarizer or len(text) < 100:\n",
    "            # Fallback: return first few sentences\n",
    "            sentences = text.split('.')[:3]\n",
    "            return '. '.join(sentences) + '.'\n",
    "        \n",
    "        try:\n",
    "            # Chunk text if too long\n",
    "            max_chunk_length = 1024\n",
    "            if len(text) > max_chunk_length:\n",
    "                text = text[:max_chunk_length]\n",
    "            \n",
    "            summary = self.summarizer(text)[0]['summary_text']\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è AI summarization failed: {e}\")\n",
    "            # Fallback summary\n",
    "            sentences = text.split('.')[:3]\n",
    "            return '. '.join(sentences) + '.'\n",
    "    \n",
    "def extract_key_topics(self, text):\n",
    "        \"\"\"Extract key topics and themes\"\"\"\n",
    "        try:\n",
    "            # Simple keyword extraction\n",
    "            words = text.lower().split()\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "            # Filter out stop words and short words\n",
    "            filtered_words = [word for word in words \n",
    "                            if word not in stop_words and len(word) > 3]\n",
    "            \n",
    "            # Count word frequency\n",
    "            word_freq = {}\n",
    "            for word in filtered_words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "            \n",
    "            # Get top 10 most frequent words as topics\n",
    "            top_topics = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            return [topic[0] for topic in top_topics]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Topic extraction failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Add the methods to the MetadataGenerator class\n",
    "MetadataGenerator.classify_document_type = classify_document_type\n",
    "MetadataGenerator.extract_entities = extract_entities\n",
    "MetadataGenerator.generate_summary = generate_summary\n",
    "MetadataGenerator.extract_key_topics = extract_key_topics\n",
    "\n",
    "print(\"‚úÖ Advanced NLP methods added to MetadataGenerator class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a65b5a7-0e47-49e0-b167-72f3a0f31620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main metadata generation method added\n"
     ]
    }
   ],
   "source": [
    "def generate_metadata(self, file_path_or_text, is_text=False):\n",
    "    \"\"\"\n",
    "    Generate comprehensive metadata for a document\n",
    "    \n",
    "    Args:\n",
    "        file_path_or_text (str): File path or raw text\n",
    "        is_text (bool): If True, treat input as raw text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive metadata\n",
    "    \"\"\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Extract text\n",
    "        if is_text:\n",
    "            text = file_path_or_text\n",
    "            filename = \"direct_text_input\"\n",
    "            file_size = len(text.encode('utf-8'))\n",
    "            file_type = \"text\"\n",
    "        else:\n",
    "            text = self.extract_text_from_file(file_path_or_text)\n",
    "            file_path = Path(file_path_or_text)\n",
    "            filename = file_path.name\n",
    "            file_size = file_path.stat().st_size\n",
    "            file_type = file_path.suffix\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(\"No text could be extracted from the document\")\n",
    "        \n",
    "        # Basic content analysis\n",
    "        word_count = len(text.split())\n",
    "        char_count = len(text)\n",
    "        \n",
    "        # Readability analysis\n",
    "        try:\n",
    "            readability_score = textstat.flesch_reading_ease(text)\n",
    "            reading_level = textstat.flesch_kincaid_grade(text)\n",
    "        except:\n",
    "            readability_score = 0\n",
    "            reading_level = 0\n",
    "        \n",
    "        # Advanced analysis (if not in quick start mode)\n",
    "        if not self.quick_start:\n",
    "            document_type = self.classify_document_type(text)\n",
    "            summary = self.generate_summary(text)\n",
    "            entities = self.extract_entities(text)\n",
    "            key_topics = self.extract_key_topics(text)\n",
    "        else:\n",
    "            document_type = \"Document\"\n",
    "            summary = text[:200] + \"...\" if len(text) > 200 else text\n",
    "            entities = {}\n",
    "            key_topics = []\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Calculate confidence score\n",
    "        confidence = self._calculate_confidence_score(text, word_count)\n",
    "        \n",
    "        # Compile metadata\n",
    "        metadata = {\n",
    "            \"basic_info\": {\n",
    "                \"filename\": filename,\n",
    "                \"file_type\": file_type,\n",
    "                \"file_size\": file_size,\n",
    "                \"processing_date\": datetime.datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": round(processing_time, 2)\n",
    "            },\n",
    "            \"content_analysis\": {\n",
    "                \"document_type\": document_type,\n",
    "                \"word_count\": word_count,\n",
    "                \"character_count\": char_count,\n",
    "                \"readability_score\": round(readability_score, 2),\n",
    "                \"reading_level\": round(reading_level, 2),\n",
    "                \"estimated_reading_time_minutes\": round(word_count / 200, 1)  # Average reading speed\n",
    "            },\n",
    "            \"semantic_data\": {\n",
    "                \"summary\": summary,\n",
    "                \"key_topics\": key_topics,\n",
    "                \"entities\": entities,\n",
    "                \"language\": \"en\"  # Could be enhanced with language detection\n",
    "            },\n",
    "            \"technical_metadata\": {\n",
    "                \"extraction_method\": \"automated\",\n",
    "                \"confidence_score\": confidence,\n",
    "                \"processing_mode\": \"quick_start\" if self.quick_start else \"full_system\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing document: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"processing_date\": datetime.datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "def _calculate_confidence_score(self, text, word_count):\n",
    "    \"\"\"Calculate confidence score based on text quality\"\"\"\n",
    "    score = 0.5  # Base score\n",
    "    \n",
    "    # Adjust based on text length\n",
    "    if word_count > 100:\n",
    "        score += 0.2\n",
    "    if word_count > 500:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Adjust based on text quality indicators\n",
    "    if len(text.split('.')) > 5:  # Multiple sentences\n",
    "        score += 0.1\n",
    "    \n",
    "    # Penalize for potential OCR issues\n",
    "    special_char_ratio = sum(1 for c in text if not c.isalnum() and c not in ' .,!?;:') / len(text)\n",
    "    if special_char_ratio > 0.1:\n",
    "        score -= 0.2\n",
    "    \n",
    "    return max(0.1, min(1.0, score))  # Clamp between 0.1 and 1.0\n",
    "\n",
    "# Add methods to the class\n",
    "MetadataGenerator.generate_metadata = generate_metadata\n",
    "MetadataGenerator._calculate_confidence_score = _calculate_confidence_score\n",
    "\n",
    "print(\"‚úÖ Main metadata generation method added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3734d38b-43c2-47d7-96d6-34933c3fad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def save_metadata_to_json(metadata, output_path):\n",
    "    \"\"\"Save metadata to JSON file\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Metadata saved to {output_path}\")\n",
    "\n",
    "def save_metadata_to_csv(metadata_list, output_path):\n",
    "    \"\"\"Save multiple metadata records to CSV\"\"\"\n",
    "    if not metadata_list:\n",
    "        print(\"‚ö†Ô∏è No metadata to save\")\n",
    "        return\n",
    "    \n",
    "    # Flatten metadata for CSV\n",
    "    flattened_data = []\n",
    "    for metadata in metadata_list:\n",
    "        if 'error' in metadata:\n",
    "            continue\n",
    "            \n",
    "        flat_record = {\n",
    "            'filename': metadata['basic_info']['filename'],\n",
    "            'file_type': metadata['basic_info']['file_type'],\n",
    "            'file_size': metadata['basic_info']['file_size'],\n",
    "            'processing_date': metadata['basic_info']['processing_date'],\n",
    "            'document_type': metadata['content_analysis']['document_type'],\n",
    "            'word_count': metadata['content_analysis']['word_count'],\n",
    "            'character_count': metadata['content_analysis']['character_count'],\n",
    "            'readability_score': metadata['content_analysis']['readability_score'],\n",
    "            'reading_level': metadata['content_analysis']['reading_level'],\n",
    "            'summary': metadata['semantic_data']['summary'][:100] + '...',  # Truncate for CSV\n",
    "            'key_topics': ', '.join(metadata['semantic_data']['key_topics'][:5]),  # Top 5 topics\n",
    "            'confidence_score': metadata['technical_metadata']['confidence_score']\n",
    "        }\n",
    "        flattened_data.append(flat_record)\n",
    "    \n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Metadata CSV saved to {output_path}\")\n",
    "\n",
    "def process_single_file(generator, file_path):\n",
    "    \"\"\"Process a single file and return metadata\"\"\"\n",
    "    print(f\"üìÑ Processing: {file_path}\")\n",
    "    metadata = generator.generate_metadata(file_path)\n",
    "    \n",
    "    if 'error' not in metadata:\n",
    "        print(f\"‚úÖ Successfully processed: {file_path}\")\n",
    "        print(f\"   Document Type: {metadata['content_analysis']['document_type']}\")\n",
    "        print(f\"   Word Count: {metadata['content_analysis']['word_count']}\")\n",
    "        print(f\"   Confidence Score: {metadata['technical_metadata']['confidence_score']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to process: {file_path}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def process_batch_files(generator, folder_path):\n",
    "    \"\"\"Process all supported files in a folder\"\"\"\n",
    "    supported_extensions = ['.pdf', '.docx', '.txt', '.md']\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    files = [f for f in folder.iterdir() \n",
    "             if f.is_file() and f.suffix.lower() in supported_extensions]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"‚ö†Ô∏è No supported files found in {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÅ Found {len(files)} files to process\")\n",
    "    \n",
    "    metadata_list = []\n",
    "    for file_path in files:\n",
    "        metadata = process_single_file(generator, file_path)\n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    return metadata_list\n",
    "\n",
    "print(\"‚úÖ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff762b2-0b39-4ac4-9600-85b36547be62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Automated Metadata Generation System\n",
      "==================================================\n",
      "\n",
      "üìã Choose System Mode:\n",
      "1. Full System (with AI models)\n",
      "2. Quick Start (basic processing)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Initializing system in Full mode...\n",
      "‚úÖ spaCy model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summarization model loaded\n",
      "‚úÖ MetadataGenerator initialized (Full System mode)\n",
      "\n",
      "==================================================\n",
      "üìã Choose Processing Mode:\n",
      "1. Process Single File\n",
      "2. Process Batch Files\n",
      "3. Process Direct Text\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1-4):  1\n",
      "Enter file path:  /home/navya/Downloads/resume_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing: /home/navya/Downloads/resume_f.pdf\n",
      "PyMuPDF extraction failed: module 'fitz' has no attribute 'open'\n",
      "‚úÖ Successfully processed: /home/navya/Downloads/resume_f.pdf\n",
      "   Document Type: Manual\n",
      "   Word Count: 376\n",
      "   Confidence Score: 0.7999999999999999\n",
      "\n",
      "üìä Metadata Summary:\n",
      "   üìÑ File: resume_f.pdf\n",
      "   üìù Type: Manual\n",
      "   üìè Words: 376\n",
      "   üìñ Summary: Cadet | National Cadet Corps, IIT Roorkee. Participated in the Guard of Honor held on Republic Day 2...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Save metadata? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìã Choose Processing Mode:\n",
      "1. Process Single File\n",
      "2. Process Batch Files\n",
      "3. Process Direct Text\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1-4):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def run_interactive_cli():\n",
    "    \"\"\"Run the interactive command line interface\"\"\"\n",
    "    print(\"üöÄ Automated Metadata Generation System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Choose system mode\n",
    "    while True:\n",
    "        print(\"\\nüìã Choose System Mode:\")\n",
    "        print(\"1. Full System (with AI models)\")\n",
    "        print(\"2. Quick Start (basic processing)\")\n",
    "        \n",
    "        mode_choice = input(\"Enter your choice (1 or 2): \").strip()\n",
    "        \n",
    "        if mode_choice == \"1\":\n",
    "            quick_start = False\n",
    "            break\n",
    "        elif mode_choice == \"2\":\n",
    "            quick_start = True\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice. Please enter 1 or 2.\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    print(f\"\\nüîß Initializing system in {'Full' if not quick_start else 'Quick Start'} mode...\")\n",
    "    generator = MetadataGenerator(quick_start=quick_start)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üìã Choose Processing Mode:\")\n",
    "        print(\"1. Process Single File\")\n",
    "        print(\"2. Process Batch Files\")\n",
    "        print(\"3. Process Direct Text\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"Enter your choice (1-4): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            # Single file processing\n",
    "            file_path = input(\"Enter file path: \").strip().strip('\"\\'')\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(\"‚ùå File not found!\")\n",
    "                continue\n",
    "            \n",
    "            metadata = process_single_file(generator, file_path)\n",
    "            \n",
    "            # Display results\n",
    "            if 'error' not in metadata:\n",
    "                print(\"\\nüìä Metadata Summary:\")\n",
    "                print(f\"   üìÑ File: {metadata['basic_info']['filename']}\")\n",
    "                print(f\"   üìù Type: {metadata['content_analysis']['document_type']}\")\n",
    "                print(f\"   üìè Words: {metadata['content_analysis']['word_count']}\")\n",
    "                print(f\"   üìñ Summary: {metadata['semantic_data']['summary'][:100]}...\")\n",
    "                \n",
    "                # Save option\n",
    "                save_choice = input(\"\\nüíæ Save metadata? (y/n): \").strip().lower()\n",
    "                if save_choice == 'y':\n",
    "                    output_path = f\"metadata_{Path(file_path).stem}.json\"\n",
    "                    save_metadata_to_json(metadata, output_path)\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            # Batch processing\n",
    "            folder_path = input(\"Enter folder path: \").strip().strip('\"\\'')\n",
    "            \n",
    "            if not os.path.exists(folder_path):\n",
    "                print(\"‚ùå Folder not found!\")\n",
    "                continue\n",
    "            \n",
    "            metadata_list = process_batch_files(generator, folder_path)\n",
    "            \n",
    "            if metadata_list:\n",
    "                successful = len([m for m in metadata_list if 'error' not in m])\n",
    "                print(f\"\\nüìä Batch Processing Complete!\")\n",
    "                print(f\"   ‚úÖ Successfully processed: {successful}/{len(metadata_list)} files\")\n",
    "                \n",
    "                # Save options\n",
    "                save_choice = input(\"\\nüíæ Save all metadata? (y/n): \").strip().lower()\n",
    "                if save_choice == 'y':\n",
    "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    \n",
    "                    # Save JSON\n",
    "                    json_path = f\"batch_metadata_{timestamp}.json\"\n",
    "                    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(metadata_list, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    # Save CSV\n",
    "                    csv_path = f\"batch_metadata_{timestamp}.csv\"\n",
    "                    save_metadata_to_csv(metadata_list, csv_path)\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            # Direct text processing\n",
    "            print(\"Enter your text (press Enter twice to finish):\")\n",
    "            lines = []\n",
    "            while True:\n",
    "                line = input()\n",
    "                if line == \"\" and lines and lines[-1] == \"\":\n",
    "                    break\n",
    "                lines.append(line)\n",
    "            \n",
    "            text = '\\n'.join(lines[:-1])  # Remove the last empty line\n",
    "            \n",
    "            if text.strip():\n",
    "                metadata = generator.generate_metadata(text, is_text=True)\n",
    "                \n",
    "                if 'error' not in metadata:\n",
    "                    print(\"\\nüìä Metadata Summary:\")\n",
    "                    print(f\"   üìù Type: {metadata['content_analysis']['document_type']}\")\n",
    "                    print(f\"   üìè Words: {metadata['content_analysis']['word_count']}\")\n",
    "                    print(f\"   üìñ Summary: {metadata['semantic_data']['summary']}\")\n",
    "                    \n",
    "                    # Save option\n",
    "                    save_choice = input(\"\\nüíæ Save metadata? (y/n): \").strip().lower()\n",
    "                    if save_choice == 'y':\n",
    "                        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                        output_path = f\"text_metadata_{timestamp}.json\"\n",
    "                        save_metadata_to_json(metadata, output_path)\n",
    "            else:\n",
    "                print(\"‚ùå No text entered!\")\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice. Please enter 1-4.\")\n",
    "\n",
    "# Run the interactive CLI\n",
    "run_interactive_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de79f915-7074-4b0d-98ce-325a69724759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Running Demo with Sample Text...\n",
      "\n",
      "============================================================\n",
      "Testing Quick Start Mode\n",
      "============================================================\n",
      "‚úÖ MetadataGenerator initialized (Quick Start mode)\n",
      "‚úÖ Demo successful!\n",
      "üìù Document Type: Document\n",
      "üìè Word Count: 142\n",
      "üìä Readability Score: -2.81\n",
      "üéØ Confidence Score: 0.7999999999999999\n",
      "üìñ Summary: \n",
      "    Artificial Intelligence in Document Processing: A Comprehensive Analysis\n",
      "    \n",
      "    This research paper examines the transformative impact of artif...\n",
      "\n",
      "============================================================\n",
      "Testing Full System Mode\n",
      "============================================================\n",
      "‚úÖ spaCy model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summarization model loaded\n",
      "‚úÖ MetadataGenerator initialized (Full System mode)\n",
      "‚úÖ Demo successful!\n",
      "üìù Document Type: Legal\n",
      "üìè Word Count: 142\n",
      "üìä Readability Score: -2.81\n",
      "üéØ Confidence Score: 0.7999999999999999\n",
      "üìñ Summary: The study analyzes various machine learning approaches including natural language processing, optical character recognition, and semantic analysis for...\n",
      "üè∑Ô∏è Key Topics: document, research, processing, artificial, intelligence\n",
      "üë• Entities found:\n",
      "   ORG: Artificial Intelligence in Document Processing, AI, Stanford University\n",
      "   PERCENT: 75%, 90%\n",
      "   PERSON: John Smith\n",
      "   DATE: January 2024 to March 2025\n"
     ]
    }
   ],
   "source": [
    "# Demo with sample text\n",
    "def run_demo():\n",
    "    \"\"\"Run a demonstration with sample text\"\"\"\n",
    "    print(\"üéØ Running Demo with Sample Text...\")\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Artificial Intelligence in Document Processing: A Comprehensive Analysis\n",
    "    \n",
    "    This research paper examines the transformative impact of artificial intelligence \n",
    "    technologies on automated document processing workflows. The study analyzes various \n",
    "    machine learning approaches including natural language processing, optical character \n",
    "    recognition, and semantic analysis for enhancing document digitization and metadata \n",
    "    extraction processes.\n",
    "    \n",
    "    Our methodology involved testing multiple AI models across diverse document types \n",
    "    including legal contracts, research papers, technical manuals, and business reports. \n",
    "    The findings demonstrate significant improvements in processing accuracy and efficiency \n",
    "    when compared to traditional manual methods.\n",
    "    \n",
    "    Key findings include a 75% reduction in processing time and 90% improvement in metadata \n",
    "    accuracy. The implementation of transformer-based models showed particular promise for \n",
    "    complex document understanding tasks.\n",
    "    \n",
    "    This research was conducted by Dr. John Smith from Stanford University in collaboration \n",
    "    with the MIT AI Lab. The study period spanned from January 2024 to March 2025.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test both modes\n",
    "    for quick_start in [True, False]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {'Quick Start' if quick_start else 'Full System'} Mode\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        generator = MetadataGenerator(quick_start=quick_start)\n",
    "        metadata = generator.generate_metadata(sample_text, is_text=True)\n",
    "        \n",
    "        if 'error' not in metadata:\n",
    "            print(\"‚úÖ Demo successful!\")\n",
    "            print(f\"üìù Document Type: {metadata['content_analysis']['document_type']}\")\n",
    "            print(f\"üìè Word Count: {metadata['content_analysis']['word_count']}\")\n",
    "            print(f\"üìä Readability Score: {metadata['content_analysis']['readability_score']}\")\n",
    "            print(f\"üéØ Confidence Score: {metadata['technical_metadata']['confidence_score']}\")\n",
    "            print(f\"üìñ Summary: {metadata['semantic_data']['summary'][:150]}...\")\n",
    "            \n",
    "            if metadata['semantic_data']['key_topics']:\n",
    "                print(f\"üè∑Ô∏è Key Topics: {', '.join(metadata['semantic_data']['key_topics'][:5])}\")\n",
    "            \n",
    "            if metadata['semantic_data']['entities']:\n",
    "                print(\"üë• Entities found:\")\n",
    "                for entity_type, entities in metadata['semantic_data']['entities'].items():\n",
    "                    if entities:\n",
    "                        print(f\"   {entity_type}: {', '.join(entities[:3])}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Demo failed: {metadata['error']}\")\n",
    "\n",
    "# Run the demo\n",
    "run_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596fe94-3d4a-48ae-9fcb-07b60c2781c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
